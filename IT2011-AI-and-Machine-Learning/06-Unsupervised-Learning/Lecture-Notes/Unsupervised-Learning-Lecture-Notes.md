# LECTURE 6 - Introduction to Unsupervised Learning

**Module:** IT2011 - Artificial Intelligence and Machine Learning  
**Lecturer:** Dr. Lakmini Abeywardhana  
**Academic Year:** Year 02, Semester 01  
**Institution:** Sri Lanka Institute of Information Technology (SLIIT)  
**Faculty:** Faculty of Computing  
**Student:** IT24103352  
**Date:** 2025-01-23

---

## Table of Contents

- [What is Unsupervised Learning?](#what-is-unsupervised-learning)
- [K-Means Clustering](#k-means-clustering)
- [Hierarchical Clustering](#hierarchical-clustering)
- [Other Clustering Methods](#other-clustering-methods)

---

## What is Unsupervised Learning?

### Definition

**Unsupervised learning** is a type of machine learning where the algorithm is trained on **unlabeled data**, meaning the data does not come with predefined outputs or categories.

**Key Characteristic:**  
The system tries to **discover hidden patterns, structures, or relationships** within the data on its own.

---

### Comparison: Supervised vs Unsupervised

```
SUPERVISED LEARNING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Features + Labels      â”‚
â”‚  Example: (Height, Weight) â†’ Classâ”‚
â”‚  Goal: Learn mapping           â”‚
â”‚  Output: Predictions           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

UNSUPERVISED LEARNING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Features only          â”‚
â”‚  Example: (Height, Weight)     â”‚
â”‚  Goal: Find patterns           â”‚
â”‚  Output: Groups/Structures     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Differences

| Aspect | Supervised Learning | Unsupervised Learning |
|--------|--------------------|-----------------------|
| **Data** | Labeled (with answers) | Unlabeled (no answers) |
| **Goal** | Predict output | Discover patterns |
| **Examples** | Classification, Regression | Clustering, Dimensionality Reduction |
| **Guidance** | Teacher provides feedback | Self-guided discovery |
| **Use Cases** | Spam detection, Price prediction | Customer segmentation, Anomaly detection |

---

### Types of Unsupervised Learning

```
Unsupervised Learning
â”‚
â”œâ”€â”€ 1. Clustering
â”‚   â”œâ”€â”€ K-Means
â”‚   â”œâ”€â”€ Hierarchical
â”‚   â”œâ”€â”€ DBSCAN
â”‚   â””â”€â”€ Gaussian Mixture Models
â”‚
â”œâ”€â”€ 2. Dimensionality Reduction
â”‚   â”œâ”€â”€ PCA (Principal Component Analysis)
â”‚   â”œâ”€â”€ t-SNE
â”‚   â””â”€â”€ Autoencoders
â”‚
â””â”€â”€ 3. Association Rule Learning
    â”œâ”€â”€ Apriori Algorithm
    â””â”€â”€ Market Basket Analysis
```

---

## K-Means Clustering

### Overview

**Definition:**  
K-means clustering is a popular unsupervised machine learning algorithm used for **data segmentation** and **grouping similar data points** into clusters.

**Applications:**
- ğŸ“Š Data analysis
- ğŸ–¼ï¸ Image processing
- ğŸ‘¥ Customer segmentation
- ğŸ“ Location-based grouping
- ğŸ·ï¸ Product categorization

---

### What is K?

**K = Number of clusters**

```
Example:
  K = 2 â†’ Two groups
  K = 3 â†’ Three groups
  K = 5 â†’ Five groups
```

**Key Question:**  
How many groups should we divide the data into?

---

## K-Means Algorithm - Step by Step

### Example Data Visualization

**Initial Data:**
```
      y
  6   â”‚    â—
      â”‚  â—   â—
  4   â”‚â—      
      â”‚    â—
  2   â”‚  â—  â—
      â”‚ â—
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
      0  2  4  6  8

8 data points scattered in 2D space
```

---

### Step 1: Initialization

**Randomly select K initial centroids**

**For K=2:**
```
      y
  6   â”‚    â—
      â”‚  â—   â—
  4   â”‚â—   C1ğŸ”´  
      â”‚    â—
  2   â”‚  â—  â—      C2ğŸŸ¢
      â”‚ â—
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
      0  2  4  6  8

C1 = Red centroid (randomly placed)
C2 = Green centroid (randomly placed)
```

---

### Step 2: Assignment

**Assign each point to nearest centroid**

**Method:**  
Calculate **Euclidean distance** from each point to each centroid.

**Formula:**
```
d = âˆš[(xâ‚‚ - xâ‚)Â² + (yâ‚‚ - yâ‚)Â²]
```

**Result:**
```
      y
  6   â”‚    ğŸ”´
      â”‚  ğŸ”´   ğŸ”´
  4   â”‚ğŸ”´   C1  
      â”‚    ğŸŸ¢
  2   â”‚  ğŸŸ¢  ğŸŸ¢      C2
      â”‚ ğŸŸ¢
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
      0  2  4  6  8

Red points assigned to C1
Green points assigned to C2
```

---

### Step 3: Update Centroids

**Recalculate centroids** by taking the **mean** of all points in each cluster.

**New Centroid Position:**
```
New C1 = Mean of all red points
New C2 = Mean of all green points
```

**Visualization:**
```
      y
  6   â”‚    ğŸ”´
      â”‚  ğŸ”´   ğŸ”´
  4   â”‚ğŸ”´   âœ–ï¸  
      â”‚    ğŸŸ¢
  2   â”‚  ğŸŸ¢  ğŸŸ¢      â•
      â”‚ ğŸŸ¢
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
      0  2  4  6  8

âœ–ï¸ = New C1 (moved to center of red cluster)
â• = New C2 (moved to center of green cluster)
```

---

### Step 4: Repeat

**Iterate Steps 2 and 3** until stopping criteria met.

**Final Result:**
```
      y
  6   â”‚    ğŸ”´
      â”‚  ğŸ”´   ğŸ”´
  4   â”‚ğŸ”´   âœ–ï¸  
      â”‚
  2   â”‚  ğŸŸ¢  ğŸŸ¢      â•
      â”‚ ğŸŸ¢   ğŸŸ¢
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
      0  2  4  6  8

Points stabilized into final clusters
```

---

## Stopping Criteria

**Algorithm stops when:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1ï¸âƒ£  Centroids Don't Change            â”‚
â”‚  New centroids = Old centroids         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2ï¸âƒ£  Points Don't Reassign             â”‚
â”‚  All points stay in same cluster       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3ï¸âƒ£  Maximum Iterations Reached        â”‚
â”‚  Predefined limit (e.g., 100 iterations)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Example with Calculations

### Dataset

|   | X1 | X2 |
|---|----|----|
| A | 2  | 3  |
| B | 6  | 1  |
| C | 1  | 2  |
| D | 3  | 1  |
| E | 6  | 4  |

**Assumption:** K = 2

---

### Iteration 1: Initial Centroids

**Randomly select 2 points as initial centroids:**
- **Centroid 1:** Point A (2, 3)
- **Centroid 2:** Point B (6, 1)

---

### Calculate Distances

**Distance from A to all points:**
```
d(A,A) = âˆš[(2-2)Â² + (3-3)Â²] = 0
d(A,B) = âˆš[(6-2)Â² + (1-3)Â²] = âˆš[16 + 4] = âˆš20 = 4.47
d(A,C) = âˆš[(1-2)Â² + (2-3)Â²] = âˆš[1 + 1] = âˆš2 = 1.41
d(A,D) = âˆš[(3-2)Â² + (1-3)Â²] = âˆš[1 + 4] = âˆš5 = 2.24
d(A,E) = âˆš[(6-2)Â² + (4-3)Â²] = âˆš[16 + 1] = âˆš17 = 4.12
```

**Distance from B to all points:**
```
d(B,A) = 4.47
d(B,B) = 0
d(B,C) = âˆš[(1-6)Â² + (2-1)Â²] = âˆš[25 + 1] = âˆš26 = 5.10
d(B,D) = âˆš[(3-6)Â² + (1-1)Â²] = âˆš[9 + 0] = 3.00
d(B,E) = âˆš[(6-6)Â² + (4-1)Â²] = âˆš[0 + 9] = 3.00
```

---

### Distance Matrix

|     | A    | B    | C    | D    | E    |
|-----|------|------|------|------|------|
| **A** | 0    | 4.47 | 1.41 | 2.24 | 4.12 |
| **B** | 4.47 | 0    | 5.10 | 3.00 | 3.00 |

---

### Assign to Nearest Cluster

**For each point, compare distances:**

```
Point A: d(A,A)=0    < d(A,B)=4.47 â†’ Cluster 1 âœ“
Point B: d(B,B)=0    < d(B,A)=4.47 â†’ Cluster 2 âœ“
Point C: d(C,A)=1.41 < d(C,B)=5.10 â†’ Cluster 1 âœ“
Point D: d(D,A)=2.24 < d(D,B)=3.00 â†’ Cluster 1 âœ“
Point E: d(E,A)=4.12 > d(E,B)=3.00 â†’ Cluster 2 âœ“
```

**Result:**
```
Cluster 1: A, C, D
Cluster 2: B, E
```

---

### Update Centroids

**Calculate mean of each cluster:**

**Cluster 1 (A, C, D):**
```
New Centroid 1:
  X1 = (2 + 1 + 3) / 3 = 6/3 = 2.0
  X2 = (3 + 2 + 1) / 3 = 6/3 = 2.0
  â†’ C1 = (2.0, 2.0)
```

**Cluster 2 (B, E):**
```
New Centroid 2:
  X1 = (6 + 6) / 2 = 12/2 = 6.0
  X2 = (1 + 4) / 2 = 5/2 = 2.5
  â†’ C2 = (6.0, 2.5)
```

**New Centroids:**

|            | X1  | X2  |
|------------|-----|-----|
| Centroid 1 | 2.0 | 2.0 |
| Centroid 2 | 6.0 | 2.5 |

---

### Iteration 2: Recalculate Distances

**Distance from C1 (2.0, 2.0) to all points:**
```
d(C1,A) = âˆš[(2-2)Â² + (3-2)Â²] = 1.00
d(C1,B) = âˆš[(6-2)Â² + (1-2)Â²] = âˆš[16+1] = 4.12
d(C1,C) = âˆš[(1-2)Â² + (2-2)Â²] = 1.00
d(C1,D) = âˆš[(3-2)Â² + (1-2)Â²] = âˆš[1+1] = 1.41
d(C1,E) = âˆš[(6-2)Â² + (4-2)Â²] = âˆš[16+4] = 4.47
```

**Distance from C2 (6.0, 2.5) to all points:**
```
d(C2,A) = âˆš[(2-6)Â² + (3-2.5)Â²] = âˆš[16+0.25] = 4.03
d(C2,B) = âˆš[(6-6)Â² + (1-2.5)Â²] = âˆš[0+2.25] = 1.50
d(C2,C) = âˆš[(1-6)Â² + (2-2.5)Â²] = âˆš[25+0.25] = 5.02
d(C2,D) = âˆš[(3-6)Â² + (1-2.5)Â²] = âˆš[9+2.25] = 3.35
d(C2,E) = âˆš[(6-6)Â² + (4-2.5)Â²] = âˆš[0+2.25] = 1.50
```

---

### Distance Matrix (Iteration 2)

|      | A    | B    | C    | D    | E    |
|------|------|------|------|------|------|
| **C1** | 1.00 | 4.12 | 1.00 | 1.41 | 4.47 |
| **C2** | 4.03 | 1.50 | 5.02 | 3.35 | 1.50 |

---

### Assign to Nearest Cluster (Iteration 2)

```
Point A: d(A,C1)=1.00 < d(A,C2)=4.03 â†’ Cluster 1 âœ“
Point B: d(B,C1)=4.12 > d(B,C2)=1.50 â†’ Cluster 2 âœ“
Point C: d(C,C1)=1.00 < d(C,C2)=5.02 â†’ Cluster 1 âœ“
Point D: d(D,C1)=1.41 < d(D,C2)=3.35 â†’ Cluster 1 âœ“
Point E: d(E,C1)=4.47 > d(E,C2)=1.50 â†’ Cluster 2 âœ“
```

**Result:**
```
Cluster 1: A, C, D (same as before)
Cluster 2: B, E     (same as before)
```

**Centroids also remain the same!**

**âœ… STOP:** Clusters are stable.

---

## Evaluating Clustering

### Challenge

**Problem:**  
Unlike supervised learning, there's no clear "accuracy" metric (no labels to compare against).

**Solution:**  
Use internal validation metrics:
1. **Inertia**
2. **Dunn Index**

---

### 1. Inertia (Within-Cluster Sum of Squares)

**Definition:**  
Sum of **squared distances** of all points within a cluster from the centroid of that cluster.

**Formula:**
```
Inertia = Î£ Î£ ||xáµ¢ - câ±¼||Â²

where:
  xáµ¢ = data point i
  câ±¼ = centroid of cluster j
  ||Â·|| = Euclidean distance
```

**Visualization:**
```
        â—
    â—   âœ–ï¸   â—
        â—

Intra-cluster distance:
Distance from each point (â—) to centroid (âœ–ï¸)
```

**Interpretation:**
- **Lower inertia** = tighter clusters (better)
- **Higher inertia** = spread out clusters (worse)

**Python Code:**
```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3)
kmeans.fit(X)

inertia = kmeans.inertia_
print(f"Inertia: {inertia}")
```

---

### 2. Dunn Index

**Definition:**  
Ratio of **minimum inter-cluster distance** to **maximum intra-cluster distance**.

**Formula:**
```
Dunn Index = min(Inter-cluster distance) / max(Intra-cluster distance)
```

**Components:**

#### Intra-cluster Distance:
```
        â—
    â—   âœ–ï¸   â—  â† Distance within cluster
        â—
```

#### Inter-cluster Distance:
```
    â—â—â—              â—‹â—‹â—‹
     âœ–ï¸  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  âŠ•
    â—â—â—              â—‹â—‹â—‹

Distance between two cluster centroids
```

---

### Dunn Index Interpretation

```
Higher Dunn Index = Better clustering

Why?
- Larger distance between clusters (well-separated)
- Smaller distance within clusters (compact)
```

**Example:**
```
Dunn Index = 0.8 â†’ Good separation, compact clusters
Dunn Index = 0.2 â†’ Poor separation, spread clusters
```

---

## Selecting Optimal K (Elbow Method)

### The Problem

**Question:**  
How many clusters (K) should we use?

**Answer:**  
Use the **Elbow Method**.

---

### Elbow Method Process

**Steps:**
1. Try different values of K (e.g., K = 1, 2, 3, ..., 10)
2. Calculate **inertia** for each K
3. Plot inertia vs K
4. Look for the "elbow point"

---

### Elbow Plot

```
Inertia
  â”‚
  â”‚â—
  â”‚ â—
  â”‚  â—
  â”‚   â—
  â”‚    â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â—
  â”‚           â­• Elbow Point
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ K
   1  2  3  4  5  6  7  8  9  10

At K=3: Curve starts to flatten
â†’ Optimal K = 3
```

**Interpretation:**
```
Before Elbow: Adding clusters significantly reduces inertia
After Elbow:  Diminishing returns (not much improvement)
```

---

### Why Elbow Point?

**Trade-off:**

```
Too Few Clusters (K small):
- High inertia
- Underfitting
- Clusters too broad

Optimal K (at elbow):
- Balanced inertia
- Good fit
- Meaningful clusters

Too Many Clusters (K large):
- Low inertia but...
- Overfitting
- Too fragmented
```

---

## Hierarchical Clustering

### Overview

**Definition:**  
Hierarchical clustering is a data analysis technique that organizes data objects into a **tree-like structure** based on their similarity.

**Applications:**
- ğŸ§¬ Biology (taxonomy, phylogenetics)
- ğŸ‘¥ Social sciences (community detection)
- ğŸ–¼ï¸ Image analysis (segmentation)
- ğŸ“Š Understanding data hierarchies

---

### Two Types

#### 1. Agglomerative (Bottom-Up) â¬†ï¸

**Most commonly used**

**Process:**
```
Start: Individual points
  A    B    C

Step 1: Merge closest
  (A,C)  B

Step 2: Continue merging
  (A,C,B)

End: Single cluster
```

**Approach:** Start small, build up

---

#### 2. Divisive (Top-Down) â¬‡ï¸

**Less common**

**Process:**
```
Start: All points together
  (A,B,C)

Step 1: Split into groups
  (A,B)  C

Step 2: Continue splitting
  A  B  C

End: Individual points
```

**Approach:** Start big, break down

---

## Agglomerative Hierarchical Clustering - Example

### Dataset

|   | X1   | X2   |
|---|------|------|
| A | 0.40 | 0.53 |
| B | 0.22 | 0.38 |
| C | 0.35 | 0.32 |
| D | 0.26 | 0.19 |
| E | 0.08 | 0.41 |
| F | 0.45 | 0.30 |

---

### Visualization

```
X2
0.6 â”‚
    â”‚
0.5 â”‚  Aâ‘ 
    â”‚
0.4 â”‚     Eâ‘¤  Bâ‘¡
    â”‚
0.3 â”‚        Câ‘¢ Fâ‘¥
    â”‚
0.2 â”‚     Dâ‘£
    â”‚
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ X1
    0.0  0.2  0.4  0.6
```

---

### Step 1: Calculate Distance Matrix

**Using Euclidean distance:**

|   | A    | B    | C    | D    | E    | F    |
|---|------|------|------|------|------|------|
| **A** | 0    |      |      |      |      |      |
| **B** | 0.23 | 0    |      |      |      |      |
| **C** | 0.22 | 0.15 | 0    |      |      |      |
| **D** | 0.37 | 0.20 | 0.15 | 0    |      |      |
| **E** | 0.34 | 0.14 | 0.28 | 0.29 | 0    |      |
| **F** | 0.23 | 0.25 | 0.11 | 0.22 | 0.39 | 0    |

**Minimum distance:** 0.11 (between C and F)

---

### Step 2: Merge C and F

**First merge:** C and F â†’ (C,F)

**New distance matrix needed:**

|       | A    | B    | (C,F) | D    | E    |
|-------|------|------|-------|------|------|
| **A**     | 0    |      |       |      |      |
| **B**     | 0.23 | 0    |       |      |      |
| **(C,F)** | ?    | ?    | 0     |      |      |
| **D**     | 0.37 | 0.20 | ?     | 0    |      |
| **E**     | 0.34 | 0.14 | ?     | 0.29 | 0    |

**Question:**  
How to calculate distance from (C,F) to other points?

---

## Linkage Methods

**Three common methods to calculate distance between clusters:**

### 1. Single Linkage (Minimum)
```
Distance = Minimum distance between any two points

d(A, (C,F)) = Min(d(A,C), d(A,F))
```

**Use case:** Finding connected components

---

### 2. Complete Linkage (Maximum)
```
Distance = Maximum distance between any two points

d(A, (C,F)) = Max(d(A,C), d(A,F))
```

**Use case:** Compact clusters

---

### 3. Average Linkage (Mean)
```
Distance = Average of all pairwise distances

d(A, (C,F)) = Avg(d(A,C), d(A,F))
```

**Use case:** Balanced approach

---

### Comparison Table

| Linkage | Example Calculation | Characteristics |
|---------|---------------------|-----------------|
| **Single** | Min(d[A,C], d[A,F]) | âœ… Handles non-spherical shapes<br>âŒ Sensitive to outliers |
| **Complete** | Max(d[A,C], d[A,F]) | âœ… Creates compact clusters<br>âŒ Sensitive to outliers |
| **Average** | Avg(d[A,C], d[A,F]) | âœ… Balanced, robust<br>âœ… Most commonly used |

---

## Example Continued (Using Single Linkage)

### Calculate New Distances

**For (C,F) to A:**
```
d(A, (C,F)) = Min(d(A,C), d(A,F))
             = Min(0.22, 0.23)
             = 0.22
```

**For (C,F) to B:**
```
d(B, (C,F)) = Min(d(B,C), d(B,F))
             = Min(0.15, 0.25)
             = 0.15
```

**For (C,F) to D:**
```
d(D, (C,F)) = Min(d(D,C), d(D,F))
             = Min(0.15, 0.22)
             = 0.15
```

**For (C,F) to E:**
```
d(E, (C,F)) = Min(d(E,C), d(E,F))
             = Min(0.28, 0.39)
             = 0.28
```

---

### Updated Distance Matrix

|       | A    | B    | (C,F) | D    | E    |
|-------|------|------|-------|------|------|
| **A**     | 0    |      |       |      |      |
| **B**     | 0.23 | 0    |       |      |      |
| **(C,F)** | 0.22 | 0.15 | 0     |      |      |
| **D**     | 0.37 | 0.20 | 0.15  | 0    |      |
| **E**     | 0.34 | 0.14 | 0.28  | 0.29 | 0    |

**Minimum distance:** 0.14 (between B and E)

---

### Step 3: Merge B and E

**Second merge:** B and E â†’ (B,E)

|         | A    | (B,E) | (C,F) | D    |
|---------|------|-------|-------|------|
| **A**       | 0    |       |       |      |
| **(B,E)**   | 0.23 | 0     |       |      |
| **(C,F)**   | 0.22 | 0.15  | 0     |      |
| **D**       | 0.37 | 0.20  | 0.15  | 0    |

**Minimum distance:** 0.15 (between (B,E) and (C,F), or (C,F) and D)

---

### Step 4: Continue Merging

**Third merge:** (B,E) and (C,F) â†’ (B,E,C,F)

|             | A    | (B,E,C,F) | D    |
|-------------|------|-----------|------|
| **A**           | 0    |           |      |
| **(B,E,C,F)**   | 0.22 | 0         |      |
| **D**           | 0.37 | 0.15      | 0    |

---

**Fourth merge:** (B,E,C,F) and D â†’ (B,E,C,F,D)

|               | A    | (B,E,C,F,D) |
|---------------|------|-------------|
| **A**             | 0    |             |
| **(B,E,C,F,D)**   | 0.22 | 0           |

---

**Final merge:** All points â†’ (A,B,C,D,E,F)

|                   | (A,B,C,D,E,F) |
|-------------------|---------------|
| **(A,B,C,D,E,F)** | 0             |

---

## Dendrogram

### What is a Dendrogram?

**Definition:**  
A tree diagram showing the hierarchical relationship between clusters.

**Reading a Dendrogram:**
```
Height = Distance at which clusters merge
Width  = Individual data points or clusters
```

---

### Example Dendrogram

```
Distance
   â”‚
0.4â”‚
   â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
0.3â”‚     â”Œâ”€â”€â”€â”¤        â”‚
   â”‚     â”‚   â”‚        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”
0.2â”‚  â”Œâ”€â”€â”¤   â”‚        â”‚        â”‚
   â”‚  â”‚  â”‚   â”‚        â”‚        â”‚
0.1â”‚ â”€â”´â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€
   â””â”€ 3  6  2  5      4        1
      C  F  B  E      D        A

Merging order:
1. C and F merge at distance 0.11
2. B and E merge at distance 0.14
3. (B,E) and (C,F) merge at distance 0.15
4. (B,E,C,F) and D merge at distance 0.15
5. Everything merges with A at distance 0.22
```

---

### Cutting the Dendrogram

**To get K clusters:**  
Draw a horizontal line at desired height.

```
Distance
   â”‚
0.3â”‚     â”Œâ”€â”€â”€â”        â”€â”€â”€â”€â”€â”€â”€â”€â”€  Cut here
   â”‚  â”Œâ”€â”€â”¤   â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”   for K=2
0.2â”‚  â”‚  â”‚   â”‚      â”‚        â”‚
   â”‚  â”‚  â”‚   â”‚      â”‚        â”‚
0.1â”‚ â”€â”´â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€
   â””â”€ 3  6  2  5    4        1

Result: 2 clusters
  Cluster 1: {C, F, B, E, D}
  Cluster 2: {A}
```

---

## Other Clustering Methods

### 1. DBSCAN (Density-Based Spatial Clustering)

**Full Name:**  
Density-Based Spatial Clustering of Applications with Noise

**Key Features:**
- âœ… Groups together points that are **densely packed**
- âœ… Can find **arbitrarily shaped** clusters
- âœ… **Identifies outliers** (noise points)
- âœ… No need to specify number of clusters

**How it Works:**
```
Core Points:   Points with many neighbors
Border Points: Near core points but fewer neighbors
Noise Points:  Not enough neighbors (outliers)
```

**Parameters:**
- **Îµ (epsilon):** Radius to search for neighbors
- **MinPts:** Minimum points to form cluster

**Example Use Case:**
```
Detecting unusual customer spending patterns
- Normal customers form dense clusters
- Fraudulent transactions appear as noise points
```

---

### 2. OPTICS (Ordering Points To Identify Clustering Structure)

**Key Features:**
- Extension of DBSCAN
- âœ… Handles clusters of **varying density** better
- âœ… Builds a **cluster ordering** rather than fixed clusters
- âœ… More flexible than DBSCAN

**Advantage:**
```
DBSCAN:  Fixed density threshold
OPTICS:  Adapts to different densities
```

**Visualization:**
```
Reachability Plot:
â”‚    â•±â•²
â”‚   â•±  â•²    â•±â•²
â”‚  â•±    â•²  â•±  â•²
â”‚ â•±      â•²â•±    â•²
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Valleys = Clusters
Peaks = Gaps between clusters
```

---

### 3. Mean Shift Clustering

**Key Features:**
- âœ… Finds **"peaks"** (high-density regions) in data
- âœ… **Doesn't need** the number of clusters in advance
- âœ… Can find arbitrarily shaped clusters

**How it Works:**
```
1. Start with random points
2. Shift each point towards area of highest density
3. Points that converge to same peak = same cluster
```

**Analogy:**
```
Imagine rolling balls down a hilly landscape.
All balls ending in same valley belong to same cluster.
```

**Applications:**
- Image segmentation
- Object tracking in videos
- Feature analysis

---

### 4. Gaussian Mixture Models (GMMs)

**Key Features:**
- **Probabilistic model** where each cluster follows a Gaussian (normal) distribution
- âœ… Provides **soft clustering** (a point can partly belong to multiple clusters)
- âœ… Flexible cluster shapes (elliptical)

**Soft Clustering Example:**
```
Point P:
  70% probability in Cluster 1
  30% probability in Cluster 2

(vs hard clustering: 100% in one cluster)
```

**Mathematical Foundation:**
```
Each cluster k has:
- Mean (Î¼â‚–)
- Covariance (Î£â‚–)
- Weight (Ï€â‚–)

P(x) = Î£ Ï€â‚– N(x | Î¼â‚–, Î£â‚–)
```

**Applications:**
- Speaker recognition
- Customer segmentation
- Image classification

---

### 5. Spectral Clustering

**Key Features:**
- Uses **graph theory** and **eigenvalues** of similarity matrix
- âœ… Very powerful for **non-convex** clusters
- âœ… Can find clusters that aren't spherical

**Process:**
```
1. Create similarity graph
2. Compute graph Laplacian
3. Find eigenvectors
4. Cluster in reduced space
```

**Strength:**
```
Can separate these clusters:
    â—â—â—â—â—
   â—     â—
  â—   â—‹â—‹â—‹ â—
   â—     â—
    â—â—â—â—â—

(Concentric circles)
```

**Applications:**
- Social network analysis
- Image segmentation
- Community detection

---

### 6. BIRCH (Balanced Iterative Reducing and Clustering)

**Full Name:**  
Balanced Iterative Reducing and Clustering using Hierarchies

**Key Features:**
- âœ… Handles **large datasets** efficiently
- âœ… Builds a tree structure (CF tree)
- âœ… Clusters data **incrementally**
- âœ… Memory efficient

**How it Works:**
```
1. Scan dataset once
2. Build Clustering Feature (CF) Tree
3. Apply clustering algorithm to CF Tree
```

**CF (Clustering Feature):**
```
CF = (N, LS, SS)
where:
  N  = Number of points
  LS = Linear sum of points
  SS = Sum of squared points
```

**Advantage:**
```
Traditional K-Means: Requires multiple full scans
BIRCH:              Single scan + tree operations
                    â†’ Much faster for large data
```

**Applications:**
- Large-scale data mining
- Real-time clustering
- Stream data processing

---

## Clustering Methods Comparison

### Summary Table

| Method | Cluster Shape | Handles Outliers | K Required? | Best For |
|--------|---------------|------------------|-------------|----------|
| **K-Means** | Spherical | âŒ No | âœ… Yes | Large datasets, spherical clusters |
| **Hierarchical** | Any | âš ï¸ Somewhat | âŒ No | Small datasets, dendrograms |
| **DBSCAN** | Arbitrary | âœ… Yes | âŒ No | Non-spherical, noisy data |
| **OPTICS** | Arbitrary | âœ… Yes | âŒ No | Varying density |
| **Mean Shift** | Arbitrary | âœ… Yes | âŒ No | Image segmentation |
| **GMM** | Elliptical | âš ï¸ Somewhat | âœ… Yes | Soft clustering, probabilistic |
| **Spectral** | Non-convex | âš ï¸ Somewhat | âœ… Yes | Complex shapes |
| **BIRCH** | Spherical | âš ï¸ Somewhat | âœ… Yes | Very large datasets |

---

### Choosing the Right Algorithm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Use K-MEANS when:                     â”‚
â”‚  - Data is numeric and spherical       â”‚
â”‚  - You know K                          â”‚
â”‚  - Speed is important                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Use HIERARCHICAL when:                â”‚
â”‚  - Small dataset                       â”‚
â”‚  - Want to visualize dendrogram        â”‚
â”‚  - Don't know K                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Use DBSCAN when:                      â”‚
â”‚  - Arbitrary cluster shapes            â”‚
â”‚  - Noisy data with outliers            â”‚
â”‚  - Don't know K                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Use GMM when:                         â”‚
â”‚  - Need probabilistic assignments      â”‚
â”‚  - Elliptical clusters                 â”‚
â”‚  - Soft clustering required            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

### Key Concepts

```
UNSUPERVISED LEARNING:
â”œâ”€â”€ No labels in training data
â”œâ”€â”€ Discovers hidden patterns
â””â”€â”€ Main task: Clustering

K-MEANS:
â”œâ”€â”€ Partition-based
â”œâ”€â”€ Fast and simple
â”œâ”€â”€ Requires K
â””â”€â”€ Spherical clusters

HIERARCHICAL:
â”œâ”€â”€ Creates tree structure
â”œâ”€â”€ No K required
â”œâ”€â”€ Dendrogram visualization
â””â”€â”€ Two types: Agglomerative & Divisive

OTHER METHODS:
â”œâ”€â”€ DBSCAN (density-based)
â”œâ”€â”€ GMM (probabilistic)
â”œâ”€â”€ Spectral (graph-based)
â””â”€â”€ Mean Shift (mode-seeking)
```

---

## Key Takeaways

### When to Use Unsupervised Learning

```
âœ… Customer segmentation
âœ… Anomaly detection
âœ… Data exploration
âœ… Feature engineering
âœ… Dimensionality reduction
âœ… Pattern discovery
```

### Algorithm Selection Flowchart

```
Do you know K?
â”œâ”€ YES â†’ Spherical clusters?
â”‚        â”œâ”€ YES â†’ K-Means
â”‚        â””â”€ NO  â†’ GMM or Spectral
â”‚
â””â”€ NO  â†’ Need hierarchy?
         â”œâ”€ YES â†’ Hierarchical
         â””â”€ NO  â†’ Arbitrary shapes?
                  â”œâ”€ YES â†’ DBSCAN
                  â””â”€ NO  â†’ Mean Shift
```

---

## Thank You!

### Course Information
- **Module:** IT2011 - Artificial Intelligence and Machine Learning
- **Lecture:** 6 - Introduction to Unsupervised Learning
- **Lecturer:** Dr. Lakmini Abeywardhana
- **Institution:** SLIIT - Sri Lanka Institute of Information Technology
- **Faculty:** Faculty of Computing
- **Academic Year:** Year 02, Semester 01
- **Student:** IT24103352
- **Date:** 2025-01-23

---

### Next Steps

**What's Coming:**
- Lecture 7: Neural Networks and Deep Learning
- Practical implementation of clustering algorithms
- Advanced unsupervised techniques

**Recommended Practice:**
1. âœ… Implement K-Means from scratch
2. âœ… Experiment with different K values
3. âœ… Try hierarchical clustering with different linkages
4. âœ… Compare algorithms on real datasets
5. âœ… Visualize clustering results

---

**End of Lecture 6**

**Total Pages:** 42  
**Completion Status:** âœ… Fully converted to Markdown
