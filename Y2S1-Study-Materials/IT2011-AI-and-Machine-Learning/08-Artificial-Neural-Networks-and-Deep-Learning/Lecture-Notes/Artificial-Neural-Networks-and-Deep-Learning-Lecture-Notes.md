# LECTURE 8 - Artificial Neural Networks and Deep Learning

**Module:** IT2011 - Artificial Intelligence and Machine Learning  
**Lecturer:** Dr. Lakmini Abeywardhana  
**Academic Year:** Year 02, Semester 01  
**Institution:** Sri Lanka Institute of Information Technology (SLIIT)  
**Faculty:** Faculty of Computing  
**Student:** IT24103352  
**Date:** 2025-10-23  
**Current Time (UTC):** 2025-10-23 10:34:50

---

## Table of Contents

- [What is Deep Learning? (Recap)](#what-is-deep-learning-recap)
- [Biological Inspiration](#biological-inspiration)
- [Introduction to Neural Networks](#introduction-to-neural-networks)
- [Perceptron Detailed](#perceptron-detailed)
- [Batch Perceptron Algorithm](#batch-perceptron-algorithm)
- [Loss Optimization](#loss-optimization)
- [Gradient Descent](#gradient-descent)
- [Activation Functions](#activation-functions)
- [Multi-Layer Neural Networks](#multi-layer-neural-networks)
- [How Neural Networks Learn](#how-neural-networks-learn)
- [Forward Propagation](#forward-propagation)
- [Loss Functions](#loss-functions)
- [Backpropagation](#backpropagation)
- [Learning Rate](#learning-rate)
- [Building Neural Networks (Practical)](#building-neural-networks-practical)
- [Overfitting and Regularization](#overfitting-and-regularization)
- [Types of Neural Networks](#types-of-neural-networks)

---

## What is Deep Learning? (Recap)

### Relationship with Machine Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â”‚
â”‚         ML (Machine Learning)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                          â”‚     â”‚
â”‚  â”‚   DL (Deep Learning)     â”‚     â”‚
â”‚  â”‚   Concepts built on      â”‚     â”‚
â”‚  â”‚   top of ML              â”‚     â”‚
â”‚  â”‚                          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Key Concepts

**Building Blocks:**
- **Perceptron** = Single neuron
- **Deep Learning** = Many perceptrons organized in layers

---

### Comparison

#### Traditional ML:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cooking with a recipe             â”‚
â”‚  Steps are predefined              â”‚
â”‚                                    â”‚
â”‚  Process:                          â”‚
â”‚  1. Manually select features       â”‚
â”‚     Example: Detect edges in image â”‚
â”‚  2. Define transformations         â”‚
â”‚  3. Train model                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Deep Learning:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chef who experiments              â”‚
â”‚  Learns by tasting                 â”‚
â”‚  Improves over time                â”‚
â”‚                                    â”‚
â”‚  Process:                          â”‚
â”‚  1. Learns features automatically  â”‚
â”‚     Example: Finds eyes, nose,     â”‚
â”‚              face without coding   â”‚
â”‚  2. Discovers patterns on its own  â”‚
â”‚  3. Improves with more data        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Biological Inspiration

### Biological Neuron

```
    Dendrites
       â•±â”‚â•²
      â•± â”‚ â•²
     â—â”€â”€â—â”€â”€â—  â† Receives signals
        â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Cell Bodyâ”‚  â† Processes information
   â”‚(Nucleus)â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
     Axonâ”‚  â† Transmits signals
        â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Synapse â”‚  â† Connection to next neuron
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Components:**
- **Dendrites:** Receive incoming signals
- **Cell Body (Soma):** Processes information
- **Nucleus:** Contains cell's genetic material
- **Axon:** Long fiber transmitting output signals
- **Synapse:** Junction connecting to other neurons

---

### Artificial Neuron Model

```
Input Signals â†’ Weights â†’ Weighted Sum â†’ Activation â†’ Output

xâ‚ â”€â”€â”€â”€â”€â”€wâ‚â”€â”€â”€â”€â”
xâ‚‚ â”€â”€â”€â”€â”€â”€wâ‚‚â”€â”€â”€â”€â”¤
xâ‚ƒ â”€â”€â”€â”€â”€â”€wâ‚ƒâ”€â”€â”€â”€â”œâ”€â”€â†’ Î£ wáµ¢xáµ¢ â†’ Ï†(Â·) â†’ Output
xâ‚„ â”€â”€â”€â”€â”€â”€wâ‚„â”€â”€â”€â”€â”˜
```

**Mathematical Process:**

**Step 1: Weighted Sum**
```
Î£(i=1 to 4) wáµ¢xáµ¢
```

**Step 2: Nonlinear Activation**
```
Output = Ï†(Î£ wáµ¢xáµ¢)
```

---

### Linear Separability

**Key Concept:**  
Biological neurons inspired artificial neurons, but single neurons can only solve **linearly separable** problems.

```
LINEARLY SEPARABLE:
  Class 1: â—â—â—
           â”€â”€â”€â”€â”€â”€  â† Can draw a line
  Class 2: â—‹â—‹â—‹
  
  âœ… Single neuron works

NON-LINEARLY SEPARABLE:
  â— â—‹ â—
  â—‹ â— â—‹
  â— â—‹ â—
  
  âŒ Single neuron fails
  âœ… Need multiple layers
```

---

## Introduction to Neural Networks

### What is a Neural Network?

**Definition:**  
A computational model inspired by biological neural networks in the human brain.

**Structure:**  
Made up of **layers** of interconnected **nodes** or **"neurons"**.

---

## Neuron Model (Detailed)

### Mathematical Representation

```
Inputs â†’ Weights â†’ Summation â†’ Bias â†’ Activation â†’ Output

xâ‚ â”€â”€â”€â”€â”€W_{k1}â”€â”€â”€â”€â”
xâ‚‚ â”€â”€â”€â”€â”€W_{k2}â”€â”€â”€â”€â”¤
   â‹®        â‹®      â”œâ”€â”€â†’ Î£ â”€â”€â†’ + bâ‚– â”€â”€â†’ Ï†(Â·) â”€â”€â†’ yâ‚–
xâ‚˜ â”€â”€â”€â”€â”€W_{km}â”€â”€â”€â”€â”˜

Where:
  xáµ¢      = Input signals
  W_{ki}  = Synaptic weights
  Î£       = Summation junction
  bâ‚–      = Bias
  Ï†(Â·)    = Activation function (signum)
  yâ‚–      = Output
```

---

### Step-by-Step Process

#### Step 1: Weighted Sum
```
uâ‚– = Î£(j=1 to m) W_{kj} Ã— xâ±¼
```

#### Step 2: Add Bias
```
vâ‚– = uâ‚– + bâ‚–
```

#### Step 3: Activation Function (Signum)
```
yâ‚– = Sgn(vâ‚–) = Ï†(uâ‚– + bâ‚–)

where:
  Sgn(vâ‚–) = {
    -1  if vâ‚– < 0
     0  if vâ‚– = 0
    +1  if vâ‚– > 0
  }
```

---

### Example Calculation

**Given:**
```
uâ‚– + bâ‚– = vâ‚– = 0.67
```

**Question:**  
What is Sgn(vâ‚–)?

**Answer:**
```
Since vâ‚– = 0.67 > 0
â†’ Sgn(0.67) = +1
```

---

## Perceptron Detailed

### Linear Separability Revisited

#### Case (a): Linearly Separable âœ…

```
      Class Câ‚
    â—  â—  â—  â—
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Decision Boundary (Hyperplane)
                     
    â—‹  â—‹  â—‹  â—‹
      Class Câ‚‚

Mathematical Representation:
  y = 1  if (wÂ·x + b) â‰¥ 0
  y = 0  otherwise
  
âœ… Perceptron can solve this
```

---

#### Case (b): Non-Linearly Separable âŒ

```
    â—  â—
  â—  â—‹  â—‹  â—
    â—‹  â—
  â—  â—‹  â—‹  â—
    â—  â—

Cannot draw a straight line to separate

âŒ Perceptron CANNOT solve this
âœ… Need advanced (multi-layer) networks
```

**Key Insight:**  
When classes Câ‚ and Câ‚‚ are too close or intermingled, they become **nonlinearly separable** â€” beyond the perceptron's capability.

---

## Perceptron Convergence Algorithm

### Variables and Parameters

```
x(n) = (m+1)-by-1 input vector
     = [+1, xâ‚(n), xâ‚‚(n), ..., xâ‚˜(n)]áµ€

w(n) = (m+1)-by-1 weight vector
     = [b, wâ‚(n), wâ‚‚(n), ..., wâ‚˜(n)]áµ€
     where b = bias

y(n) = actual response (predicted, quantized)
d(n) = desired response (actual result)
Î·    = learning rate (0 < Î· < 1)
```

---

### Algorithm Steps

#### Step 1: Initialization
```
Set w(0) = [0, 0, ..., 0]áµ€
(All weights start at zero)
```

---

#### Step 2: Activation
```
At time-step n:
- Apply input vector x(n)
- Apply desired response d(n)
```

**Note:**  
This is for **linearly separable data**.

---

#### Step 3: Compute Actual Response
```
y(n) = sgn[wáµ€(n)x(n)]

where sgn() is the signum function:
  sgn(z) = {
    +1  if z â‰¥ 0
    -1  if z < 0
  }
```

**Interpretation:**
- Calculates **predicted result**
- Compares with **actual result**

---

#### Step 4: Adaptation of Weight Vector
```
w(n+1) = w(n) + Î·[d(n) - y(n)]x(n)

where desired response:
  d(n) = {
    +1  if x(n) belongs to class Ï†â‚
    -1  if x(n) belongs to class Ï†â‚‚
  }
```

**Update Logic:**
```
If prediction correct: d(n) = y(n)
  â†’ d(n) - y(n) = 0
  â†’ No weight update

If prediction wrong: d(n) â‰  y(n)
  â†’ d(n) - y(n) â‰  0
  â†’ Adjust weights
```

---

#### Step 5: Continuation
```
n = n + 1 (increment time step)
Go back to Step 2
```

**Repeat until:**
- All samples correctly classified, OR
- Maximum iterations reached

---

### Example: Perceptron Training

**Dataset:**

| xâ‚ | xâ‚‚ | Class |
|----|----|----|
| 1 | 1.5 | +1 |
| 1.5 | 2.5 | +1 |
| 2 | 1 | +1 |
| 2.5 | 4 | -1 |
| 3 | 2.5 | -1 |
| 5 | 2 | -1 |

---

**Iteration 0: Initialize**

```
x(0) = [+1, +1, +1.5]áµ€  (first row with bias)
w(0) = [0, 0, 0]áµ€       (all zeros)
```

**Compute:**
```
v = wáµ€x = [0, 0, 0] Â· [+1, +1, +1.5]
  = 0Ã—1 + 0Ã—1 + 0Ã—1.5
  = 0

y(0) = sgn(v) = sgn(0) = 0
```

**Desired:**
```
d(0) = +1  (from dataset)
```

**Update:**
```
w(1) = w(0) + Î·[d(0) - y(0)]x(0)
     = [0,0,0] + Î·[1-0][1,1,1.5]
     = Î·[1,1,1.5]
```

---

**After Training (Final Weights):**

| b | wâ‚ | wâ‚‚ |
|---|----|----|
| 1.8 | -0.3 | -0.5 |

**Decision Boundary:**
```
1.8 - 0.3xâ‚ - 0.5xâ‚‚ = 0

Rearranged:
xâ‚‚ = 3.6 - 0.6xâ‚
```

---

### Visualization

```
xâ‚‚
 4â”‚     Ã—       â† Class -1
  â”‚   Ã—   Ã—
 3â”‚  Ã—
  â”‚
 2â”‚    â—‹  â—‹     â† Class +1
  â”‚â—‹  â”€â”€â”€â”€â”€â”€â”€â”€  Decision Boundary
 1â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ xâ‚
   0  1  2  3  4  5

â—‹ = Class +1 (blue circles)
Ã— = Class -1 (red x's)
```

---

## Batch Perceptron Algorithm

### Motivation

**Previous algorithm:**
- âŒ No explicit cost function
- âŒ Single-sample correction

**Batch approach:**
- âœ… Uses cost function
- âœ… Updates based on **multiple samples**
- âœ… More generalized

---

### Cost Function for Perceptron

**Definition:**
```
J(w) = Î£ (-wáµ€x(n)d(n))
       x(n)âˆˆğ•

where:
  ğ• = set of MISCLASSIFIED samples
```

**Interpretation:**
```
If all samples correctly classified:
  ğ• = empty set
  J(w) = 0 âœ…

If some samples misclassified:
  J(w) > 0 âŒ
  
Number of misclassified samples â†‘
â†’ J(w) â†‘
```

---

### Understanding Misclassification

**For misclassified samples:**

```
SCENARIO 1: True class = Câ‚ (+1)
  wáµ€(n)Â·x(n) > 0  (predicted +1, correct!)
  wáµ€(n)Â·x(n) < 0  (predicted -1, WRONG!)
  
SCENARIO 2: True class = Câ‚‚ (-1)
  wáµ€(n)Â·x(n) < 0  (predicted -1, correct!)
  wáµ€(n)Â·x(n) > 0  (predicted +1, WRONG!)
```

**Unified Condition for Misclassification:**
```
wáµ€(n)Â·x(n)Â·d(n) < 0

Therefore:
-wáµ€(n)Â·x(n)Â·d(n) > 0

Cost Function:
J(w) = Î£ (-wáµ€Â·x(n)Â·d(n))
       for all misclassified samples
```

---

### Gradient Vector

**Differentiate J(w) with respect to w:**
```
âˆ‡J(w) = âˆ‚J(w)/âˆ‚w
      = Î£ (-x(n)d(n))
        x(n)âˆˆğ•
```

---

### Weight Update (Batch)

**Single Sample Update:**
```
w(n+1) = w(n) + Î·(x(n)d(n))
```

**Batch Update (All Misclassified):**
```
w(n+1) = w(n) + Î· Î£ (x(n)d(n))
                   xâˆˆğ•

where:
  ğ• = set of incorrectly classified samples
```

**Note:**  
These samples are **selected randomly** from the misclassified set.

---

## Loss Optimization

### The Optimization Problem

**Goal:**
```
Find weights W* that minimize the loss

W* = argmin_W [ (1/n) Î£ L(f(xâ½â±â¾; W), yâ½â±â¾) ]
              i=1 to n
```

**Components:**
- **W:** Weights (parameters) of the neural network
- **f(xâ½â±â¾; W):** Model's prediction for input xâ½â±â¾ using weights W
- **yâ½â±â¾:** True label for input xâ½â±â¾
- **L(Â·,Â·):** Loss function (e.g., MSE, cross-entropy)
- **(1/n) Î£ ...:** Average loss over all training examples

---

### Simplified Notation

```
W* = argmin_W J(W)

where:
  J(W) = cost function (average loss)
```

---

### Training Interpretation

**Bottom Line:**
```
Training a neural network means:

Adjusting weights W so the model's 
predictions are as CLOSE as possible 
to the true outputs, by MINIMIZING 
the loss (cost) function.
```

---

## Gradient Descent

### Visual Intuition

```
    Cost J(w)
      â”‚     â•±â•²
      â”‚   â•±    â•²
      â”‚ â•±        â•²    â† Start here
      â”‚â•±____      â•²
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Weights w
         â†“
    Roll down hill
    to find minimum
```

**Analogy:**  
Finding the lowest point in a valley by always stepping downhill.

---

### The Gradient

**Gradient = Direction of steepest ASCENT**

**Therefore:**  
Move in **opposite** direction to descend!

```
Gradient: âˆ‚J(w)/âˆ‚w  (points up)
Update:   -Î· Ã— (âˆ‚J(w)/âˆ‚w)  (go down)
```

---

### Update Rule

```
w â† w - Î· Ã— (âˆ‚J(w)/âˆ‚w)

where:
  w        = current weight
  Î·        = learning rate (step size)
  âˆ‚J(w)/âˆ‚w = gradient of loss w.r.t. weight
  â†        = assignment (update)
```

---

### Example: Single Weight

**Cost Function:**
```
J(w) = convex curve
```

**Steps:**
1. Start at weight w
2. Calculate gradient g = âˆ‚J(w)/âˆ‚w
3. Update: w_new = w - Î·Ã—g
4. Repeat until minimum

**Visualization:**
```
J(w)
 â”‚    â•±â•²
 â”‚  â•±    â•²
 â”‚â•±  w    â•²
 â”‚    â†“     â•²
 â”‚   w_new   â•²
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ w
      Gradient
      descent
      step
```

---

### Three Scenarios

```
SCENARIO A: wâ‚ (left of minimum)
  gâ‚ = negative slope
  w_x = wâ‚ - gâ‚  (moves right) âœ…
  
SCENARIO B: wâ‚‚ (closer to minimum)
  gâ‚‚ = smaller negative slope
  w_x = wâ‚‚ - gâ‚‚  (smaller step right) âœ…
  
SCENARIO C: wâ‚ƒ (right of minimum)
  gâ‚ƒ = positive slope
  w_x = wâ‚ƒ - gâ‚ƒ  (moves left) âœ…

All converge to minimum!
```

---

### Gradient Calculation

**For perceptron cost function:**
```
J(w) = -[wáµ€Â·x(n)Â·d(n)]  (for misclassified)

âˆ‚J(w)/âˆ‚w = -(x(n)d(n))
```

**Update:**
```
w(n+1) = w(n) - Î·(-x(n)d(n))
       = w(n) + Î·(x(n)d(n))
```

**Key Insight:**  
The **slope** tells you **how much to move**.

---

## Activation Functions

### Why Activation Functions?

**Without activation:**
```
Multiple linear layers = Single linear layer
(No benefit to depth!)
```

**With activation:**
```
Non-linear transformations
â†’ Can learn complex patterns
â†’ Deep networks become powerful
```

---

### 1. Sigmoid Function

**Formula:**
```
Ïƒ(x) = 1 / (1 + eâ»Ë£)
```

**Graph:**
```
Ïƒ(x)
1.0â”‚         â•­â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚       â•±
0.5â”‚     â•±
   â”‚   â•±
0.0â”‚â•­â”€â”€
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
  -10    0    10
```

**Properties:**
```
Range: (0, 1)
Interpretation: Can be used as probability

If x > 0:  0.5 â‰¤ y â‰¤ 1
If x â‰¤ 0:  0 â‰¤ y â‰¤ 0.5
```

**Use Cases:**
- Output layer (binary classification)
- Historical hidden layers (now less common)

---

### 2. Tanh Function

**Formula:**
```
tanh(x) = (eË£ - eâ»Ë£) / (eË£ + eâ»Ë£)
```

**Graph:**
```
tanh(x)
 1.0â”‚         â•­â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚       â•±
 0.0â”‚     â•±
    â”‚   â•±
-1.0â”‚â•­â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
   -10    0    10
```

**Properties:**
```
Range: (-1, 1)
Zero-centered (advantage over sigmoid)
```

---

### 3. ReLU (Rectified Linear Unit)

**Formula:**
```
ReLU(x) = max(0, x) = {
  x  if x > 0
  0  if x â‰¤ 0
}
```

**Graph:**
```
ReLU(x)
10â”‚        â•±
  â”‚      â•±
 5â”‚    â•±
  â”‚  â•±
 0â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
 -10  0  10
```

**Properties:**
```
Range: [0, âˆ)
Computationally efficient
Most popular for hidden layers

If Î£xáµ¢wáµ¢ > 0:  y = Î£xáµ¢wáµ¢
If Î£xáµ¢wáµ¢ â‰¤ 0:  y = 0
```

**Advantages:**
- âœ… Fast to compute
- âœ… No vanishing gradient (for positive values)
- âœ… Sparse activation

**Drawback:**
- âŒ "Dying ReLU" (neurons can become inactive)

---

### 4. Leaky ReLU

**Formula:**
```
Leaky ReLU(x) = {
  x      if x > 0
  Î±x     if x â‰¤ 0
}

where Î± = small constant (e.g., 0.01)
```

**Graph:**
```
Leaky ReLU(x)
10â”‚        â•±
  â”‚      â•±
 5â”‚    â•±
  â”‚  â•±
 0â”‚â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€  (slight slope for x<0)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
 -10  0  10
```

**Properties:**
```
Range: (-âˆ, âˆ)
Addresses "dying ReLU" problem
```

---

## Multi-Layer Neural Networks

### What is an MLP?

**Definition:**  
Advanced model with **multiple layers** stacked on top of each other.

**Key Difference from Perceptron:**
```
Perceptron:  Single neuron
MLP:         Multiple layers of neurons
```

---

### Types Based on Depth

#### Shallow Neural Network

```
Input Layer â†’ Hidden Layer â†’ Output Layer
              (single layer)

Characteristics:
- Usually ONE hidden layer
- Fast training
- Less processing power
- Limited complexity
```

**Example Architecture:**
```
Input Layer (4 nodes):
  xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„
  â†“  â†“  â†“  â†“
Hidden Layer (5 nodes):
  Perceptronâ‚ (Î£, Ï†)
  Perceptronâ‚‚ (Î£, Ï†)
  Perceptronâ‚ƒ (Î£, Ï†)
  Perceptronâ‚„ (Î£, Ï†)
  Perceptronâ‚… (Î£, Ï†)
  â†“
Output Layer (1 node):
  Output

Weights:
  Wâ‚â‚, Wâ‚â‚‚, Wâ‚â‚ƒ, Wâ‚â‚„ (to first hidden neuron)
  ...
```

---

#### Deep Neural Network

```
Input â†’ Hiddenâ‚ â†’ Hiddenâ‚‚ â†’ ... â†’ Hidden_N â†’ Output
        (multiple layers)

Characteristics:
- MULTIPLE hidden layers
- Slower training
- More processing power
- Can learn complex patterns
```

**Example Architecture:**
```
Input Nodes:   aâ‚, aâ‚‚, ..., a_N
  â†“
Layer 1:       bâ‚, bâ‚‚, ..., b_V
  â†“
Layer k:       kâ‚, kâ‚‚, ..., k_Z
  â†“
Layer l:       lâ‚, lâ‚‚, ..., l_W
  â†“
Output Nodes:  nâ‚, nâ‚‚, ..., n_V
```

---

### Fully Connected / Dense Layers

**Definition:**  
All inputs are **densely connected** to all outputs.

```
Layer i          Layer i+1
  â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â—
  â—â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â—
  â—â”€â”€â”¼â”€â”€â”¬â”€â”€â”€â”€â”€â”€â†’â—
  â—â”€â”€â”¼â”€â”€â”¼â”€â”€â”¬â”€â”€â”€â†’â—
     â”‚  â”‚  â”‚
Every neuron connected to every next neuron
```

---

## MLP Structure - Detailed

### Input Layer

**Purpose:**  
Takes in raw features from dataset

**Example (Image Recognition):**
```
28Ã—28 grayscale image:
  - 784 pixels
  - Each pixel = 1 input
  - Values: 0-255 (grayscale intensity)

Colored image (height Ã— width Ã— 3):
  - 3 channels: Red, Green, Blue
  - Each channel = separate inputs
```

**Characteristics:**
- âœ… No computation
- âœ… Just passes data forward
- âœ… One neuron per feature

---

### Hidden Layers

**Purpose:**  
Transform inputs into useful representations

**Process:**
```
For each hidden layer:
1. Weighted sum: z = Î£(wáµ¢xáµ¢) + b
2. Activation: a = Ï†(z)  (e.g., ReLU)
3. Pass to next layer
```

**Learning Hierarchy (Image Example):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT LAYER                   â”‚
â”‚  Raw pixel values (0-255)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HIDDEN LAYER 1                â”‚
â”‚  Detects: Low-level features   â”‚
â”‚  - Edges                       â”‚
â”‚  - Lines                       â”‚
â”‚  - Corners                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HIDDEN LAYER 2                â”‚
â”‚  Detects: Mid-level features   â”‚
â”‚  - Shapes (circles, rectangles)â”‚
â”‚  - Textures                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HIDDEN LAYER 3                â”‚
â”‚  Detects: High-level features  â”‚
â”‚  - Objects (faces, cars)       â”‚
â”‚  - Complex patterns            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT LAYER                  â”‚
â”‚  Final classification          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Output Layer

**Purpose:**  
Produces final decision/prediction

**Binary Classification:**
```
Single neuron with sigmoid:
  Output = probability of class 1
  
Example: Cat vs Not-Cat
  [0.9] â†’ 90% chance of Cat
  [0.1] â†’ 10% chance of Not-Cat
  
Decision: If P â‰¥ 0.5 â†’ Cat
          If P < 0.5 â†’ Not-Cat
```

**Multi-Class Classification:**
```
Multiple neurons with softmax:
  Output = probability distribution
  
Example: Cat/Dog classification (4 classes)
  [0.5 (cat), 0.3 (dog), 0.1, 0.1]
  Î£ = 1.0
  
Decision: argmax â†’ Cat
```

---

## Building Neural Networks (Practical)

### Example: 4 â†’ 5 â†’ 1 MLP

**Architecture:**
- Input: 4 features
- Hidden: 5 neurons (ReLU)
- Output: 1 neuron (Sigmoid)

---

### TensorFlow/Keras Implementation

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define model
model = keras.Sequential([
    layers.Input(shape=(4,)),           # Input layer: 4 features
    layers.Dense(5, activation="relu"),  # Hidden: 5 neurons (ReLU)
    layers.Dense(1, activation="sigmoid") # Output: 1 neuron (Sigmoid)
])

# Compile model
model.compile(
    optimizer="adam",                # Optimizer (adaptive learning rate)
    loss="binary_crossentropy",      # Loss function (binary classification)
    metrics=["accuracy"]             # Evaluation metric
)

# View architecture
model.summary()
```

**Output:**
```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 5)                 25        
dense_1 (Dense)              (None, 1)                 6         
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
```

---

### PyTorch Implementation

```python
import torch
import torch.nn as nn

# Define model
model = nn.Sequential(
    nn.Linear(4, 5),      # Input layer: 4 â†’ 5
    nn.ReLU(),            # Hidden activation
    nn.Linear(5, 1),      # Hidden â†’ Output: 5 â†’ 1
    nn.Sigmoid()          # Output activation
)

# View architecture
print(model)
```

---

### Visualization

```
Input Layer (4):
  â— â— â— â—
  â”‚ â”‚ â”‚ â”‚
  â””â”€â”´â”€â”´â”€â”¼â”€â”€â”€â”€â”€â”
        â”‚     â”‚
Hidden Layer (5):
  â— â— â— â— â—
  [Î£,ReLU] each
  â”‚ â”‚ â”‚ â”‚ â”‚
  â””â”€â”´â”€â”´â”€â”´â”€â”˜
        â”‚
Output Layer (1):
  â—
  [Î£,Sigmoid]
        â”‚
  Prediction
```

---

## How Neural Networks Learn

### The Learning Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: FORWARD PROPAGATION           â”‚
â”‚  Data flows: Input â†’ Hidden â†’ Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: COMPUTE LOSS                  â”‚
â”‚  Measure error                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: BACKPROPAGATION               â”‚
â”‚  Send error backward                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: ADJUST WEIGHTS                â”‚
â”‚  Use gradient descent                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Step 1: Forward Propagation

**Process:**  
Data flows layer by layer

**Example:**
```
Image (pixels) â†’ Layer 1 â†’ Layer 2 â†’ ... â†’ Prediction
```

---

### Step 2: Loss Function

**Measures error between prediction and actual**

**Example:**
```
Predicted: "Cat" (0.7)
Actual:    "Dog" (0.0 for Cat)

Error: Large! âŒ
```

---

### Step 3: Backpropagation

**Error sent backward to update weights**

Uses **Gradient Descent** to minimize error step by step

---

### Step 4: Training

**Repeat forward + backward many times**

```
Epoch 1: 1000 images â†’ calculate loss â†’ update weights
Epoch 2: Same 1000 images â†’ lower loss â†’ update weights
...
Epoch 20: Loss minimized â†’ stop
```

---

## Forward Propagation (Details)

### Idea

Data flows **layer by layer**. Each neuron:
1. Weighted sum
2. Add bias
3. Apply activation

Outputs of one layer â†’ Inputs to next

---

### Math (for layer l)

**Weights & Bias:**
```
Wâ½Ë¡â¾, bâ½Ë¡â¾
```

**Pre-Activation:**
```
zâ½Ë¡â¾ = Wâ½Ë¡â¾ aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾

where:
  aâ½Ë¡â»Â¹â¾ = activations from previous layer
```

**Activation:**
```
aâ½Ë¡â¾ = f(zâ½Ë¡â¾)

where:
  f(Â·) = activation function (ReLU, sigmoid, etc.)
```

---

### Output Layer

**Binary Classification:**
```
Å· = Ïƒ(z)  (sigmoid)

Output: Probability (0-1)
```

**Multi-Class Classification:**
```
Å· = softmax(z)

Output: Probability distribution
  Example: [0.6, 0.2, 0.1, 0.1] (sums to 1.0)
```

**Example:**
```
4-class output:
  Dog:  0.6
  Cat:  0.2
  Cow:  0.1
  Goat: 0.1
  â”€â”€â”€â”€
  Sum = 1.0 âœ“
```

---

## Loss Functions

### Purpose

**Compare prediction Å· with true label y**

The loss is a **single number to minimize**.

---

### 1. Binary Cross-Entropy (BCE)

**Use:** Binary classification (2 classes)

**Formula:**
```
L = -[y log(Å·) + (1-y)log(1-Å·)]

where:
  y  = true label (0 or 1)
  Å·  = predicted probability
```

---

### 2. Categorical Cross-Entropy (CCE)

**Use:** Multi-class classification (K classes)

**Formula:**
```
L = -Î£(k=1 to K) yâ‚– log(Å·â‚–)

where:
  yâ‚–  = true label (one-hot encoded)
  Å·â‚–  = predicted probability for class k
```

---

### 3. Mean Squared Error (MSE)

**Use:** Regression

**Formula:**
```
L = (1/n) Î£(Å· - y)Â²

where:
  n  = number of samples
  Å·  = predicted value
  y  = true value
```

---

## Gradient Descent (Detailed)

### Visualization

```
     Cost J(w)
      â”‚     â•±â•²
      â”‚   â•±    â•²
      â”‚ â•±   ?    â•²    â† Where to start?
      â”‚â•±____      â•²
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Weights
         â†“
    Roll down hill
```

---

### 3D Surface

```
J(wâ‚€, wâ‚)
   â•±â•²  â•±â•²
  â•±  â•²â•±  â•²   â† Multiple local minima
 â•±    â•±    â•²

Path: Ã— â†’ Ã— â†’ Ã— â†’ minimum
     (step by step)
```

---

## Gradient Descent Algorithm

### Pseudocode

```python
# Algorithm

# 1. Initialize weights randomly
W = random_normal(0, ÏƒÂ²)

# 2. Loop until convergence
while not converged:
    
    # 3. Compute gradient (using backpropagation)
    gradient = âˆ‚J(W)/âˆ‚W
    
    # 4. Update weights
    W = W - Î· Ã— gradient
    
# 5. Return final weights
return W
```

---

### Parameters

```
W        = current weight
Î·        = learning rate (step size)
âˆ‚J(W)/âˆ‚W = gradient of loss w.r.t. weight
```

---

### Update Visualization

```
J(w)
 â”‚    â•±â•²
 â”‚  â•±    â•²
 â”‚â•±  W    â•²
 â”‚    â†“     â•²
 â”‚   W_new   â•²
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ w

Step = Î· Ã— gradient
```

---

## Backpropagation (Details)

### Computational Graph

```
Input x â†’ Multiply by wâ‚ â†’ zâ‚ â†’ ... â†’ Å· â†’ Loss J(W)
```

**Chain Rule:**
```
âˆ‚J(W)/âˆ‚wâ‚ = (âˆ‚J(W)/âˆ‚Å·) Ã— (âˆ‚Å·/âˆ‚zâ‚) Ã— (âˆ‚zâ‚/âˆ‚wâ‚)

Propagate gradient backward through each operation
```

---

### Backpropagation Flow

```
FORWARD PASS:
  Input x â†’ Hidden â†’ Output Å· â†’ Loss L

BACKWARD PASS:
  âˆ‚L/âˆ‚Å· â† âˆ‚L/âˆ‚hidden â† âˆ‚L/âˆ‚w
  
  Backpropagate all gradients from output to input
```

---

### Example: 3-Layer Network

**Architecture:**
```
Input Layer â†’ Hidden Layer â†’ Output Layer
  (Xâ‚, Xâ‚‚)      (Nâ‚â‚, Nâ‚â‚‚, Nâ‚â‚ƒ)     (O/P)
```

**Weights:**
```
wâ‚, wâ‚ƒ, wâ‚„, wâ‚…, ... (connecting layers)
```

**Backpropagation:**
```
âˆ‚J/âˆ‚w computed layer by layer
Starting from output, working backward
```

---

## Learning Rate

### What is Learning Rate?

**Definition:**  
A hyperparameter that controls the **speed** at which a neural network learns.

**Measure:**  
How much weights are updated each time the model is trained.

---

### Values

```
Typical range: 0.000001 to 0.001

Examples:
  0.000001  (very small)
  0.0001
  0.001     (common default)
```

---

### Low Learning Rate

```
J(Î¸)
 â”‚    â•±â•²
 â”‚  â•±    â•²
 â”‚â•± â†’â†’â†’â†’â†’ â•²
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Î¸

Small steps (â†’)

Pros:
  âœ… More stable
  âœ… Less overshooting

Cons:
  âŒ Slow convergence
  âŒ May get stuck
```

---

### High Learning Rate

```
J(Î¸)
 â”‚    â•±â•²
 â”‚  â•± â†”â†” â•²
 â”‚â•±   â†”   â•²
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Î¸

Large steps (â†”)

Pros:
  âœ… Fast convergence

Cons:
  âŒ Overshoots minimum
  âŒ May diverge
```

---

### Decent Learning Rate

```
J(Î¸)
 â”‚    â•±â•²
 â”‚  â•± â†’ â•²
 â”‚â•±â†’  â†’ â†’â•²
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Î¸

Balanced steps

âœ… Good convergence
âœ… Reaches minimum
```

---

### Techniques for Handling Learning Rate

#### 1. Learning Rate Finder

**Approach:**  
Empirically search for suitable rate

**Process:**
```
Test range: 0.0001 to 1.0
Observe loss behavior
Choose rate where loss decreases fastest
```

---

#### 2. Adaptive Learning Rate Methods

**Optimizers that adjust automatically:**

```
ADAM (Adaptive Moment Estimation):
  - Adapts learning rate per parameter
  - Considers past gradients
  - Most popular

RMSprop:
  - Root Mean Square Propagation
  - Adapts based on gradient magnitude

AdaGrad:
  - Adaptive Gradient
  - Larger updates for infrequent features
```

**Advantage:**  
Adapts to the "landscape" of the loss function.

---

## Building Neural Networks - Complete Example

### Architecture: 4 â†’ 5 â†’ 1

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define model
model = keras.Sequential([
    layers.Input(shape=(4,)),           # Input: 4 features
    layers.Dense(5, activation="relu"),  # Hidden: 5 neurons (ReLU)
    layers.Dense(1, activation="sigmoid") # Output: 1 neuron (Sigmoid)
])

# Compile model
model.compile(
    optimizer="adam",              # Adaptive learning rate
    loss="binary_crossentropy",    # BCE loss (binary classification)
    metrics=["accuracy"]           # Track accuracy
)

# View architecture
model.summary()
```

---

### Visualization

```
Input:
  â— (input)
  â†“ Sig(1)
  â— (output)
```

---

### Hyperparameters

**What are hyperparameters?**
```
Parameters set BEFORE training:
  - Learning rate
  - Number of layers
  - Number of neurons per layer
  - Activation functions
  - Batch size
  - Number of epochs
```

---

## Good Practices in Neural Networks

### 1. Use Mini-Batches

**Why?**
```
âœ… More accurate gradient estimation
âœ… Faster training
âœ… Better generalization
```

**Example:**
```
Dataset: 1000 images

Batch training:
  - Process all 1000 at once
  - Slow, memory-intensive

Mini-batch training:
  - Process 20-100 images at a time
  - Update weights after each mini-batch
  - Much faster
```

---

## Overfitting and Regularization

### The Problem of Overfitting

**Definition:**  
Model learns training data **too well**, including noise.

---

### Visual Example

```
UNDERFITTING:
  Yâ”‚  â—   â—
   â”‚â—  â—
   â”‚ â—
   â””â”€â”€â”€â”€â”€â”€ X
   
  Model: â”€â”€â”€â”€â”€â”€  (straight line)
  Too simple, misses pattern

BALANCED:
  Yâ”‚  â—   â—
   â”‚â—  â—
   â”‚ â—
   â””â”€â”€â”€â”€â”€â”€ X
   
  Model: âˆ¿âˆ¿âˆ¿âˆ¿  (smooth curve)
  Captures general trend

OVERFITTING:
  Yâ”‚  â—   â—
   â”‚â—  â—
   â”‚ â—
   â””â”€â”€â”€â”€â”€â”€ X
   
  Model: âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿  (wiggly line)
  Fits every point, captures noise
```

---

## How to Avoid Overfitting

### 1. Adding Dropouts

**Concept:**  
Randomly "drop out" neurons during training

**Diagram:**
```
STANDARD NETWORK:
  â—â”€â”€â—â”€â”€â—â”€â”€â—
  â—â”€â”€â—â”€â”€â—â”€â”€â—
  â—â”€â”€â—â”€â”€â—â”€â”€â—

NETWORK WITH DROPOUT:
  â—â”€â”€â—‹â”€â”€â—â”€â”€â—  (â—‹ = dropped out)
  â—â”€â”€â—â”€â”€â—‹â”€â”€â—
  â—‹â”€â”€â—â”€â”€â—â”€â”€â—‹
```

**Implementation:**
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# 4 â†’ 4 â†’ 4 â†’ 1 with Dropout

model = keras.Sequential([
    layers.Input(shape=(4,)),
    layers.Dense(4, activation="relu"),
    layers.Dropout(0.5),                 # Drop 50% of units
    layers.Dense(4, activation="relu"),
    layers.Dropout(0.5),                 # Drop 50% again
    layers.Dense(1, activation="sigmoid")
])

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)
```

---

### 2. Early Stopping

**Concept:**  
Stop training when validation loss starts increasing

**Graph:**
```
Loss
 â”‚
 â”‚  Training Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  (keeps decreasing)
 â”‚             â•²
 â”‚              â•²
 â”‚  Validation Loss âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿âˆ¿  (starts increasing)
 â”‚             â•±     â†‘
 â”‚  Underfitting  STOP HERE  Overfitting
 â”‚                (Early stopping)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs
```

**Implementation:**
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define model
model = keras.Sequential([
    layers.Input(shape=(4,)),
    layers.Dense(4, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(4, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(1, activation="sigmoid")
])

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

# Early stopping callback
early_stop = keras.callbacks.EarlyStopping(
    monitor="val_loss",              # Watch validation loss
    patience=5,                      # Stop if no improvement for 5 epochs
    mode="min",                      # Minimize validation loss
    restore_best_weights=True        # Use best model seen during training
)

# Train with early stopping
model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    callbacks=[early_stop]
)
```

---

## Types of Neural Networks

### 1. Feedforward Neural Network (FNN)

**Architecture:**
```
Input Layer â†’ Hidden Layer â†’ Output Layer
  (Forward only, no loops)

â”Œâ”€â”€â”€â”€â”€â”
â”‚  xâ‚ â”‚â”€â”€â”
â”œâ”€â”€â”€â”€â”€â”¤  â”‚  â”Œâ”€â”€â”€â”€â”
â”‚  xâ‚‚ â”‚â”€â”€â”¼â”€â†’â”‚ hâ‚ â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”˜  â”‚  â”œâ”€â”€â”€â”€â”¤  â”‚  â”Œâ”€â”€â”€â”€â”
         â””â”€â†’â”‚ hâ‚‚ â”‚â”€â”€â”¼â”€â†’â”‚ y  â”‚
            â””â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”˜
                    â”‚
                    â””â”€â†’
```

**How it Works:**
- Information moves **one direction**
- Input â†’ Hidden â†’ Output
- **No feedback loops**

**Use Cases:**
- âœ… Basic classification (spam detection)
- âœ… Simple pattern recognition
- âœ… Function approximation

**Key Point:**  
Simplest type of neural network

---

### 2. Recurrent Neural Network (RNN)

**Architecture:**
```
       â”Œâ”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â†’â”‚Hiddenâ”‚â”€â”€â”
    â”‚  â”‚Layer â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â”‚
Input     â†“     Output
    â””â”€â”€â”€â”€Loopâ”€â”€â”€â”€â”˜
    (Recurrence)
```

**How it Works:**
- Connections **loop back**
- **Remembers** information from previous steps
- Maintains internal state (memory)

**Use Cases:**
- âœ… Language modeling
- âœ… Text prediction
- âœ… Speech recognition
- âœ… Time-series forecasting

**Key Point:**  
Good for **sequential data** where order matters

---

### 3. Long Short-Term Memory (LSTM)

**Comparison with RNN:**

```
RNN UNIT:
  Simple loop
  h_{t-1} â†’ Ïƒ â†’ h_t
  Short-term memory only

LSTM UNIT:
  Complex internal structure
  â”œâ”€â”€ Forget Gate (Ïƒ)
  â”œâ”€â”€ Input Gate (Ïƒ)
  â”œâ”€â”€ Input Modulation Gate (Ï†)
  â””â”€â”€ Output Gate (Ïƒ)
  
  c_{t-1} (cell state) â†’ c_t
  Long-term memory preserved
```

**How it Works:**
- Special type of RNN
- Avoids "forgetting" long sequences
- Uses **memory cells** and **gates**

**Use Cases:**
- âœ… Machine translation
- âœ… Chatbot responses
- âœ… Predicting stock prices

**Key Point:**  
Designed to **remember long sequences**

---

### 4. Generative Adversarial Networks (GANs)

**How it Works:**
- Two networks compete:
  1. **Generator:** Creates fake data
  2. **Discriminator:** Checks if data is real or fake

**Use Cases:**
- âœ… Creating realistic images
- âœ… Deepfakes
- âœ… Art generation
- âœ… Data augmentation

**Key Point:**  
Good at **generating new, realistic data**

---

### 5. Self-Organizing Maps (SOMs)

**How it Works:**
- Unsupervised network
- Maps **high-dimensional data** â†’ **low dimension** (2D/3D)
- Preserves topological relationships

**Use Cases:**
- âœ… Data clustering
- âœ… Feature mapping
- âœ… Pattern recognition

**Key Point:**  
Helps **visualize complex data**

---

### 6. Modular Neural Networks (MNNs)

**How it Works:**
- Splits big problem into **smaller parts**
- Each part solved by **separate network**
- Results **combined** at the end

**Use Cases:**
- âœ… Robotics
- âœ… Medical diagnosis
- âœ… Large, complex systems

**Key Point:**  
Breaks down **complex problems** into manageable modules

---

## Summary

### Key Concepts

```
DEEP LEARNING:
â”œâ”€â”€ Built on top of ML
â”œâ”€â”€ Learns features automatically
â”œâ”€â”€ Inspired by biological neurons
â””â”€â”€ Uses multiple layers

PERCEPTRON:
â”œâ”€â”€ Single neuron
â”œâ”€â”€ Linear separability limitation
â”œâ”€â”€ Foundation for deep learning
â””â”€â”€ Uses convergence algorithm

NEURAL NETWORKS:
â”œâ”€â”€ Multiple layers (input, hidden, output)
â”œâ”€â”€ Weights + Bias + Activation
â”œâ”€â”€ Forward propagation (predictions)
â”œâ”€â”€ Backpropagation (learning)
â””â”€â”€ Gradient descent (optimization)

ACTIVATION FUNCTIONS:
â”œâ”€â”€ Sigmoid (0,1) - output layer
â”œâ”€â”€ Tanh (-1,1) - hidden layers
â”œâ”€â”€ ReLU [0,âˆ) - most popular
â””â”€â”€ Leaky ReLU (-âˆ,âˆ) - addresses dying ReLU

TRAINING:
â”œâ”€â”€ Forward pass (make prediction)
â”œâ”€â”€ Compute loss (measure error)
â”œâ”€â”€ Backward pass (compute gradients)
â”œâ”€â”€ Update weights (gradient descent)
â””â”€â”€ Repeat until convergence

REGULARIZATION:
â”œâ”€â”€ Dropout (randomly drop neurons)
â”œâ”€â”€ Early stopping (stop when overfitting begins)
â”œâ”€â”€ Batch normalization
â””â”€â”€ Weight decay

TYPES OF NETWORKS:
â”œâ”€â”€ FNN (feedforward)
â”œâ”€â”€ RNN (recurrent)
â”œâ”€â”€ LSTM (long short-term memory)
â”œâ”€â”€ GAN (generative adversarial)
â”œâ”€â”€ SOM (self-organizing map)
â””â”€â”€ MNN (modular)
```

---

## Thank You!


---

### Next Steps

**Practice:**
1. âœ… Implement perceptron from scratch
2. âœ… Build multi-layer networks in TensorFlow/PyTorch
3. âœ… Experiment with different architectures
4. âœ… Understand backpropagation mathematically
5. âœ… Try different activation functions
6. âœ… Apply regularization techniques

---

**End of Lecture 8**



---

**Congratulations!** ğŸ‰  
You now have **ALL LECTURES** (including both parts of Lecture 8) fully converted to comprehensive Markdown format, ready for your GitHub repository!
